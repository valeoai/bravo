<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BRAVO Workshop @ICCV 2023</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Merriweather+Sans:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&display=swap"
        rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>
    <link href="css/style.css" rel="stylesheet">
    <!-- social media tags -->
    <meta name="description"
        content="The BRAVO Workshop @ICCV 2023 presents a unique opportunity for researchers, industry experts, and policymakers to come together and address the critical challenge of trustworthy validation for autonomous vehicle systems on open roads.">
    <meta property="og:url" content="https://valeoai.github.io/bravo/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="BRAVO Workshop @ICCV 2023">
    <meta property="og:description"
        content="The BRAVO Workshop @ICCV 2023 presents a unique opportunity for researchers, industry experts, and policymakers to come together and address the critical challenge of trustworthy validation for autonomous vehicle systems on open roads.">
    <meta property="og:image" content="https://valeoai.github.io/bravo/images/hero/preview.jpg">
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="valeoai.github.io">
    <meta property="twitter:url" content="https://valeoai.github.io/bravo/">
    <meta name="twitter:title" content="BRAVO Workshop @ICCV 2023">
    <meta name="twitter:description"
        content="The BRAVO Workshop @ICCV 2023 presents a unique opportunity for researchers, industry experts, and policymakers to come together and address the critical challenge of trustworthy validation for autonomous vehicle systems on open roads.">
    <meta name="twitter:image" content="https://valeoai.github.io/bravo/images/hero/preview.jpg">
</head>

<body>
    <div id="page-hero" class="hero bg-image vh-100">
        <a href="#main-content" class="skip-to-main-content-link">Skip to main content</a>
        <div id="header" class="container header-container">
            <h1 class="bravo-logo">
                <div class="emojiFlip">üöô</div>BRAV<span class="emoji">üåç</span>
            </h1>
            <h1>roBustness and Reliability of Autonomous <br>Vehicles in the Open-world</h1>
            <p class="subtitle">
                An <a href="https://iccv2023.thecvf.com/list.of.accepted.workshops-363300-4-31-33.php" target="_blank">
                    ICCV'23 workshop</a> &centerdot; October 3rd, 2023 &centerdot; Paris, France
            </p>

            <nav class="navbar navbar-expand-md navbar-light">
                <div class="container-fluid justify-content-center">
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
                        aria-controls="navbarNav" aria-expanded="false" aria-label="toggle navigation bar">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <div class="collapse navbar-collapse justify-content-center" id="navbarNav">
                        <ul class="navbar-nav">
                            <li class="nav-item">
                                <a class="nav-link" href="#speakers">Speakers</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#program">Program</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#challenge">Challenge</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#cfp" style="color:#FF0000">Submissions (Updated!)</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#dates">Dates</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#organizers">Organizers</a>
                            </li>
                        </ul>
                    </div>
                </div>
            </nav>
            <p class="announcement">
                The <a style="color:#FF0000" href="#challenge">BRAVO challenge </a> is online! üî•üî•üî•
            </p>
        </div>
    </div>

    <div id="main" class="container page-container">
        <div id="abstract" class="container-md section-container section-first">
            <div id="main-content">
                <p class="lead">The BRAVO workshop presents a unique opportunity for researchers, industry experts, and
                    policymakers to come together and address the critical challenge of trustworthy validation for
                    autonomous vehicle systems on open roads.</p>
                <p>The advances in artificial intelligence and computer vision propel the rise of highly automated
                    <acronym title="advanced driver-assistance systems">ADAS</acronym> and <acronym
                        title="autonomous vehicles">AVs</acronym>, with the potential to revolutionize transportation
                    and mobility services. However, deploying data-driven safety-critical systems with limited onboard
                    resources and enduring guarantees on open roads remains a significant challenge.
                </p>
                <p>To ensure safe deployment, ADAS/AVs must demonstrate the ability to navigate a wide range of driving
                    conditions, including rare and dangerous situations, severe perturbations, and even adversarial
                    attacks. Additionally, those capabilities must be ascertained to regulatory bodies, to secure
                    certification, and to users, to earn their confidence.</p>
                <p>The BRAVO workshop seeks to foster collaboration and innovation in developing tools and testbeds for
                    assessing and enhancing the robustness, generalization power, transparency, and verification of
                    computer vision models for ADAS/AVs. By working together, we can contribute to a safer, more
                    efficient, and sustainable future for transportation.</p>
                <p>We invite you to join us at the BRAVO workshop to explore solutions and contribute to developing
                    reliable, robust computer vision for autonomous vehicles. Together, we can shape the future of
                    transportation, ensuring safety and efficiency for all road users.</p>
            </div>
        </div>

        <div id="speakers" class="container-md section-container">
            <h2>Keynote Speakers</h2>
            <div id="speakers_container"
                class="d-flex flex-wrap justify-content-around align-items-center person-container">
                <div class="card person-card">
                    <a class="person-link" href="http://ai.bu.edu/ksaenko.html">
                        <div class="card-img-top framed-photo"><img src="images/people/kate.jpg" alt=""></div>
                        <div class="card-title person-name" class=>Kate Saenko</div>
                        <div class="card-text person-affiliation">Boston University</div>
                    </a>
                </div>
                <div class="card person-card">
                    <a class="person-link" href="https://cispa.saarland/group/fritz/">
                        <div class="card-img-top framed-photo"><img src="images/people/mario.jpg" alt=""></div>
                        <div class="card-title person-name">Mario Fritz</div>
                        <div class="card-text person-affiliation">CISPA Helmholtz Center<br>for Information Security
                        </div>
                    </a>
                </div>
                <div class="card person-card">
                    <a class="person-link" href="https://team.inria.fr/rits/membres/raoul-de-charette">
                        <div class="card-img-top framed-photo"><img src="images/people/raoul.jpg" alt=""></div>
                        <div class="card-title person-name">Raoul de Charette</div>
                        <div class="card-text person-affiliation">INRIA</div>
                    </a>
                </div>
                <div class="card person-card">
                    <a class="person-link" href="https://pages.cs.wisc.edu/~sharonli/">
                        <div class="card-img-top framed-photo"><img src="images/people/sharon.jpg" alt=""></div>
                        <div class="card-title person-name">Sharon Yixuan Li</div>
                        <div class="card-text person-affiliation">UW‚ÄìMadison</div>
                    </a>
                </div>
                <div class="card person-card">
                    <a class="person-link" href="http://www.tatianatommasi.com/">
                        <div class="card-img-top framed-photo"><img src="images/people/tatiana.jpg" alt=""></div>
                        <div class="card-title person-name">Tatiana Tommasi</div>
                        <div class="card-text person-affiliation">Politecnico di Torino</div>
                    </a>
                </div>
                <div class="card person-card">
                    <a class="person-link" href="https://scholar.google.com/citations?user=WuJckpkAAAAJ&hl=en">
                        <div class="card-img-top framed-photo"><img src="images/people/vijay.jpg" alt=""></div>
                        <div class="card-title person-name">Vijay Badrinarayanan</div>
                        <div class="card-text person-affiliation">Wayve</div>
                    </a>
                </div>
            </div>
        </div>

        <div id="program" class="container-md section-container">
            <h2>Tentative Program</h2>
            <p>All quoted times refer to <a href="http://heurelegalefrancaise.fr" target="_blank">CEST</a>.
            <div id="program_container" class="container program-container">
                <h3>Morning</h3>
                <div class="row program-row">
                    <div class="col-md-3 col-lg-2">08:50 - 09:00</div>
                    <div class="col-md-6 col-lg-4">Opening Remarks</div>
                    <div class="col-md-3 col-lg-2">by TBD</div>
                </div>
                <div class="row program-row">
                    <div class="col-md-3 col-lg-2">09:00 - 09:45</div>
                    <div class="col-md-6 col-lg-4">Invited Talk 1</div>
                    <div class="col-md-3 col-lg-2">by TBD</div>
                </div>
                <div class="row program-row">
                    <div class="col-md-3 col-lg-2">09:45 - 10:30</div>
                    <div class="col-md-6 col-lg-4">Invited Talk 2</div>
                    <div class="col-md-3 col-lg-2">by TBD</div>
                </div>
                <div class="row program-row">
                    <div class="col-md-3 col-lg-2">10:30 - 10:45</div>
                    <div class="col-md-6 col-lg-4">Coffee Break</div>
                </div>
                <div class="row program-row">
                    <div class="col-md-3 col-lg-2">10:45 - 11:30</div>
                    <div class="col-md-6 col-lg-4">Invited Talk 3</div>
                    <div class="col-md-3 col-lg-2">by TBD</div>
                </div>
                <div class="row program-row">
                    <div class="col-md-3 col-lg-2">11:30 - 12:15</div>
                    <div class="col-md-6 col-lg-4">Invited Talk 4</div>
                    <div class="col-md-3 col-lg-2">by TBD</div>
                </div>
                <h3>Afternoon</h3>
                <div class="row program-row">
                    <div class="col-md-3 col-lg-2">13:30 - 14:15</div>
                    <div class="col-md-6 col-lg-4">Oral Presentations</div>
                </div>
                <div class="row program-row">
                    <div class="col-md-3 col-lg-2">14:15 - 15:30</div>
                    <div class="col-md-6 col-lg-4">Coffee Break and Poster Session</div>
                </div>
                <div class="row program-row">
                    <div class="col-md-3 col-lg-2">15:30 - 16:15</div>
                    <div class="col-md-6 col-lg-4">Invited Talk 5</div>
                    <div class="col-md-3 col-lg-2">by TBD</div>
                </div>
                <div class="row program-row">
                    <div class="col-md-3 col-lg-2">16:15 - 17:00</div>
                    <div class="col-md-6 col-lg-4">Invited Talk 6</div>
                    <div class="col-md-3 col-lg-2">by TBD</div>
                </div>
                <div class="row program-row">
                    <div class="col-md-3 col-lg-2">17:00 - 17:45</div>
                    <div class="col-md-6 col-lg-4">Panel Discussion + QA</div>
                </div>
            </div>
        </div>


        <div id="cfp" class="container-md section-container">
            <h2>Accepted Submissions</h2>
            <ul class="accepted-papers">
                <li><em>A glimpse at the first results of the AutoBehave project: a multidisciplinary approach to
                        evaluate
                        the usage of our travel time in self-driving cars.</em> Carlos F Crispim-Junior, Romain
                    Guesdon,
                    Christophe Jallais, Florent Laroche, Stephanie Souche-Le Corvec, Georges Beurier, Xuguang Wang,
                    Laure Tougne Rodet. (Abstract)</li>
                <li><em>A Subdomain-Specific Knowledge Distillation Method for Unsupervised Domain Adaptation in
                        Adverse
                        Weather Conditions.</em> Yejin Lee, Gyuwon Choi, Donggon Jang, Daeshik Kim (Abstract)</li>
                <li><em>An Empirical Analysis of Range for 3D Object Detection.</em> Neehar Peri, Mengtian Li,
                    Benjamin
                    Wilson, Yu-Xiong Wang, James Hays, Deva Ramanan. (Full Paper)</li>
                <li><em>Anomaly-Aware Semantic Segmentation via Style-Aligned OoD Augmentation.</em> Dan Zhang,
                    Kaspar
                    Sakmann, William Beluch, Robin Hutmacher, Yumeng Li. (Full Paper)</li>
                <li><em>Camera-Based Road Snow Coverage Estimation.</em> Kai Cordes, Hellward Broszio.</em> (Full
                    Paper)
                </li>
                <li><em>Deep Ensembles Spread Over Time ‚Äì Enabling Deep Ensembles in Real-Time Applications.</em>
                    Isak P
                    Meding, Alexander Bodin, Adam Tonderski, Joakim Johnander, Christoffer Petersson, Lennart
                    Svensson.
                    (Full Paper)</li>
                <li><em>Fusing Pseudo Labels with Weak Supervision for Dynamic Traffic Scenarios.</em> Harshith
                    Mohan
                    Kumar, Sean Lawrence. (Abstract)</li>
                <li><em>GPS-GLASS: Learning Nighttime Semantic Segmentation Using Daytime Video and GPS data.</em>
                    Hongjae Lee, Changwoo Han, Jun-Sang Yoo, Seung-Won Jung. (Full Paper)</li>
                <li><em>Identifying Systematic Errors in Object Detectors with the SCROD Pipeline.</em> Valentyn
                    Boreiko, Matthias Hein, Jan Hendrik Metzen. (Full Paper)</li>
                <li><em>Introspection of 2D Object Detection using Processed Neural Activation Patterns in Automated
                        Driving Systems.</em> Hakan Y Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger Woodman.
                    (Full
                    Paper)
                </li>
                <li><em>On Offline Evaluation of 3D Object Detection for Autonomous Driving.</em> Tim Schreier,
                    Katrin
                    Renz, Andreas Geiger, Kashyap Chitta. (Full Paper)</li>
                <li><em>On the Interplay of Convolutional Padding and Adversarial Robustness.</em> Paul Gavrikov,
                    Janis
                    Keuper. (Full Paper)</li>
                <li><em>Sensitivity analysis of AI-based algorithms for autonomous driving on optical wavefront
                        aberrations induced by the windshield.</em> Dominik W Wolf, Markus Ulrich, Nikhil Kapoor.
                    (Full
                    Paper)</li>
                <li><em>Synthetic Dataset Acquisition for a Specific Target Domain.</em> Joshua Niemeijer, Sudhanshu
                    Mittal, Thomas Brox. (Full Paper)</li>
                <li><em>T-FFTRadNet: Object Detection with Swin Vision Transformers from Raw ADC Radar Signals.</em>
                    James Giroux, Martin Bouchard, Robert Laganiere. (Full Paper)</li>
                <li><em>Unsupervised Domain Adaptation for Self-Driving from Past Traversal Features.</em> Travis
                    Zhang,
                    Katie Z Luo, Cheng Perng Phoo, Yurong You, Mark Campbell, Bharath Hariharan, Kilian Weinberger.
                    (Full
                    Paper)
                </li>
                <li><em>What Does Really Count? Estimating Relevance of Corner Cases for Semantic Segmentation in
                        Automated Driving.</em> Jasmin Breitenstein, Florian Heidecker, Maria Lyssenko, Daniel
                    Bogdoll,
                    Maarten Bieshaar, Marius Z√∂llner, Bernhard Sick, Tim Fingscheidt. (Full Paper)</li>
            </ul>

            <h3>Reviewers</h3>
            <p>We extend our heartfelt gratitude to the team of reviewers who made this call for constributions
                possible:</p>
            <div class="container mt-5">
                <div class="row">
                    <div class="col-lg-6">
                        <!-- Column 1: Names and Institutions (1st half) -->
                        <div class="d-flex flex-column">
                            <div class="reviewer d-flex justify-content-start"><span>Adrien
                                    Lafage&nbsp;&mdash;&nbsp;</span><span>ENSTA Paris</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Alexandre
                                    Boulch&nbsp;&mdash;&nbsp;</span><span>Valeo.ai</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Alexandre
                                    Ram√©&nbsp;&mdash;&nbsp;</span><span>LIP6</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Antoine
                                    Saporta&nbsp;&mdash;&nbsp;</span><span>Meero</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Antonin
                                    Vobecky&nbsp;&mdash;&nbsp;</span><span>Valeo.ai / CTU, FEE / CIIRC</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Arthur
                                    Ouaknine&nbsp;&mdash;&nbsp;</span><span>McGill University / Mila</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>C√©dric
                                    Rommel&nbsp;&mdash;&nbsp;</span><span>Valeo.ai</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Charles
                                    Corbiere&nbsp;&mdash;&nbsp;</span><span>Valeo.ai</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>David
                                    Hurych&nbsp;&mdash;&nbsp;</span><span>Valeo.ai</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Dmitry
                                    Kangin&nbsp;&mdash;&nbsp;</span><span>Lancaster University</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Eduard
                                    Zamfir&nbsp;&mdash;&nbsp;</span><span>University of Wurzburg</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Emanuel
                                    Aldea&nbsp;&mdash;&nbsp;</span><span>Paris-Saclay University</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Emilie
                                    Wirbel&nbsp;&mdash;&nbsp;</span><span>Nvidia</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Fabio
                                    Arnez&nbsp;&mdash;&nbsp;</span><span>Universit√© Paris-Saclay, CEA, List</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Fabio
                                    Pizzati&nbsp;&mdash;&nbsp;</span><span>University of Oxford</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Fredrik
                                    Gustafsson&nbsp;&mdash;&nbsp;</span><span>Uppsala University</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Himalaya
                                    Jain&nbsp;&mdash;&nbsp;</span><span>Helsing</span></div>
                        </div>
                    </div>
                    <div class="col-lg-6">
                        <!-- Column 2: Names and Institutions (2nd half) -->
                        <div class="d-flex flex-column">
                            <div class="reviewer d-flex justify-content-start"><span>Krzysztof
                                    Lis&nbsp;&mdash;&nbsp;</span><span>EPFL</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Lo√Øck
                                    Chambon&nbsp;&mdash;&nbsp;</span><span>Valeo.ai</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Matej
                                    Grciƒá&nbsp;&mdash;&nbsp;</span><span>University of Zagreb</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Matthieu
                                    Cord&nbsp;&mdash;&nbsp;</span><span>Sorbonne University</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Maximilian
                                    Jaritz&nbsp;&mdash;&nbsp;</span><span>Amazon</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Mickael
                                    Chen&nbsp;&mdash;&nbsp;</span><span>Valeo.ai</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Nazir Nayal&nbsp;&mdash;&nbsp;
                                </span><span>Ko√ß University</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Olivier
                                    Laurent&nbsp;&mdash;&nbsp;</span><span>Universit√© Paris-Saclay</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Oriane
                                    Sim√©oni&nbsp;&mdash;&nbsp;</span><span>Valeo.ai</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Patrick
                                    P√©rez&nbsp;&mdash;&nbsp;</span><span>Valeo.ai</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Pau de Jorge
                                    Aranda&nbsp;&mdash;&nbsp;</span><span>University of Oxford</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Raffaello
                                    Camoriano&nbsp;&mdash;&nbsp;</span><span>Politecnico di Torino</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Raoul de
                                    Charette&nbsp;&mdash;&nbsp;</span><span>Inria</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Renaud
                                    Marlet&nbsp;&mdash;&nbsp;</span><span>Valeo.ai / √âcole des Ponts ParisTech</span>
                            </div>
                            <div class="reviewer d-flex justify-content-start"><span>Riccardo Volpi&nbsp;&mdash;&nbsp;
                                </span><span>Naver Labs</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Spyros
                                    Gidaris&nbsp;&mdash;&nbsp;</span><span>Valeo.ai</span></div>
                            <div class="reviewer d-flex justify-content-start"><span>Suha
                                    Kwak&nbsp;&mdash;&nbsp;</span><span>POSTECH</span></div>
                        </div>
                    </div>
                </div>
            </div>
            <p style="margin-top: 1em">...and three other reviewers who preferred to remain anonymous.</p>

            <h2 style="margin-top: 1em;"><s>Call for Contributions</s></h2>
            <p>We invite participants to submit their work to the BRAVO Workshop as full papers or extended abstracts.
            </p>
            <h3>Full-Paper Submissions</h3>
            <p>Full papers must present original research, not published elsewhere, and follow the <a
                    href="https://iccv2023.thecvf.com/submission.guidelines-361600-2-20-16.php" target="_blank">ICCV
                    main conference format</a> with a length of 4 to 8 pages (extra pages with references only are
                allowed). Supplemental materials are <b>not</b> allowed. Accepted full papers will be included in the
                conference proceedings.
            </p>
            <h3>Extended Abstract Submissions</h3>
            <p>We welcome extended abstracts, which may serve works of a more speculative or preliminary nature that may
                not be fit for a full-length paper. Authors are also welcome to submit extended abstracts for previously
                or concomitantly published works that could foster the workshop objectives.
            <p>
            <p>Extended abstracts must have no more than 1000 words, in addition to a single illustration and
                references. We suggest authors use the <a href="files/extended-abstract-template.zip" download>extended
                    abstract template</a> provided.</p>
            <p>Accepted extended abstracts will be presented <b>without</b> inclusion in the proceedings.</p>
            <h3>Topics of Interest</h3>
            <p>The workshop welcomes submissions on all topics related to robustness, generalization, transparency, and
                verification of computer vision for autonomous driving systems. Topics of interest include but are not
                limited to:</p>
            <ol>
                <li>Robustness & Domain Generalization</li>
                <li>Domain Adaptation & Shift</li>
                <li>Long-tail Recognition</li>
                <li>Perception in Adverse Conditions</li>
                <li>Out-of-distribution Detection</li>
                <li>Applications of Uncertainty Quantification</li>
                <li>Monitoring, Failure Prediction & Anomaly Detection</li>
                <li>Confidence Calibration</li>
                <li>Image Enhancement Techniques</li>
            </ol>
            <h3>Guidelines</h3>
            <p>All submissions must be made through the <a href="https://cmt3.research.microsoft.com/BRAVO2023"
                    target="_blank">CMT system</a>, before the <a href=" #dates">deadline</a>.</p>
            <p>The BRAVO Workshop reviewing is <b>double-blind</b>. Authors of all submissions must
                follow the <a href="https://iccv2023.thecvf.com/policies-361500-2-20-15.php" target="_blank">main
                    conference policy on anonymity</a>. We encourage authors to follow the <a
                    href="https://iccv2023.thecvf.com/suggested.practices.for.authors-362500-2-24-25.php"
                    target="_blank"> ICCV 2023 Suggested Practices for Authors</a>, except in what concerns supplemental
                material, which is not allowed.</p>
            <p>While we encourage reproducibility, we welcome preliminary/speculative works where source codes or data
                might need more time until broad disclosure. We still expect evidence of ethics clearance if the
                submission uses novel data sources from human subjects.</p>
            <p>BRAVO Workshop reviewers must follow the <a
                    href="https://iccv2023.thecvf.com/ethics.for.reviewing.papers-362100-2-16-21.php"
                    target="_blank">ICCV 2023 Ethics Guidelines for Reviewers</a>. We encourage reviewers to follow the
                <a href="https://iccv2023.thecvf.com/reviewer.guidelines-362000-2-16-20.php" target="_blank">ICCV 2023
                    Reviewer Guidelines</a>, and <a
                    href="https://iccv2023.thecvf.com/additional.tips.for.writing.good.reviews-362200-2-16-22.php"
                    target="_blank">Tips to Write Good Reviews</a>.
            </p>
            <h3>Camera-ready instructions</h3>
            <p>The submission guidelines are detailed <a
                    href="https://docs.google.com/document/d/1Bo6JagywppxKc1TbGaCpnip3PXxmDaW3KU8coUlM98E/edit?usp=sharing">here</a>.
            </p>
        </div>

        <div id="dates" class="container-md section-container">
            <h2>Important Dates</h2>
            <div id="dates_container" class="container dates-container">
                <div class="row dates-row">
                    <div class="col-md-3 col-lg-2">
                        <span class="announce_date"><b>2023-07-20 Thu</b></span>
                    </div>
                    <div class="col-md-9">
                        <b><a href="https://www.timeanddate.com/countdown/generic?iso=20230720T235959&p0=769&msg=BRAVO+Worshop+%2723+Submissions+Deadline&font=slab&csz=1"
                                target="_blank">Contributed submissions deadline (23:59 GMT)</a></b>
                    </div>
                </div>
                <div class="row dates-row">
                    <div class="col-md-3 col-lg-2">
                        <span class="announce_date">2023-08-03 Thu</span>
                    </div>
                    <div class="col-md-9">
                        Acceptance of contributions announced to authors
                    </div>
                </div>
                <div class="row dates-row">
                    <div class="col-md-3 col-lg-2">
                        <span class="announce_date" style="color:#FF0000">2023-08-20 Sun</span>
                    </div>
                    <div class="col-md-9">
                        Camera-ready submissions deadline
                    </div>
                </div>
                <div class="row dates-row">
                    <div class="col-md-3 col-lg-2">
                        <span class="announce_date">2023-10-03 Tue</span>
                    </div>
                    <div class="col-md-9">
                        Workshop day (full day)
                    </div>
                </div>
            </div>
        </div>

        <div id="challenge" class="container-md section-container">
            <h2>BRAVO Challenge</h2>
            <p>In conjunction with the BRAVO workshop at ICCV'23, we are organizing a challenge on the robustness of
                autonomous driving in the open world. The 2023 BRAVO Challenge aims at benchmarking segmentation models
                on urban scenes undergoing diverse forms of natural degradation and realistic-looking synthetic
                corruptions. We offer two tracks for benchmarking segmentation models: (1) trained on a single dataset
                and (2) trained on multiple heterogeneous datasets.</p>

            <h3>General rules</h3>
            <ol>
                <li>Models in each track must be trained using only the datasets allowed for that track.</li>
                <li>It is strictly forbidden to employ generative models for synthetic data augmentation.</li>
                <li>All results must be reproducible. Participants must submit a white paper containing comprehensive
                    technical details alongside their results. Participants must make models and inference code
                    accessible.</li>
            </ol>

            <h3>Track 1 ‚Äì Single-domain training</h3>
            <p>In this track, models must be trained exclusively on the published
                <a href="https://www.cityscapes-dataset.com/">Cityscapes dataset</a>. This track evaluates the
                robustness of models trained with limited supervision and geographical diversity when facing unexpected
                corruptions observed in real-world scenarios.
            </p>

            <p>The evaluation will be performed on the 19 semantic classes of Cityscapes.
            </p>

            <h3>Track 2 ‚Äì Multi-domain training</h3>
            <p>In this track, the models may be trained over a mix of multiple datasets, whose choice is strictly
                limited to the list provided below, comprising both real and synthetic domains. This track aims to
                assess how fewer constraints on the training data can enhance robustness.</p>

            <p>The evaluation will be performed on the 19 semantic classes of
                <a href="https://www.cityscapes-dataset.com/">Cityscapes</a>. Participants may choose to maintain
                the label sets of each dataset or remap them to Cityscapes.
            </p>

            <p>Allowed training datasets for Track 2:</p>
            <ul>
                <li><a href="https://www.cityscapes-dataset.com/¬†">Cityscapes</a></li>
                <li><a href="https://bdd-data.berkeley.edu/">BDD100k</a></li>
                <li><a href="https://www.mapillary.com/datasets">Mapillary Vistas</a></li>
                <li><a href="https://idd.insaan.iiit.ac.in/">India Driving Dataset</a></li>
                <li><a href="https://www.wilddash.cc/">WildDash 2</a></li>
                <li><a href="https://download.visinf.tu-darmstadt.de/data/from_games/">GTA5 Dataset</a> (synthetic)</li>
                <li><a href="https://www.vis.xyz/shift/">SHIFT Dataset</a> (synthetic)</li>
            </ul>

            <h3>BRAVO Dataset</h3>
            <p>We created the benchmark dataset with real captured images and realistic-looking augmented images,
                repurposing existing datasets and combining them with newly generated data. The benchmark dataset
                comprises images from <a href="https://acdc.vision.ee.ethz.ch/">ACDC</a>,
                <a href="https://segmentmeifyoucan.com/">SegmentMeIfYouCan</a>,
                <a href="https://arxiv.org/abs/2108.00968">Out-of-context Cityscapes</a>, and new synthetic data.
            </p>
            <p>Get the full benchmark dataset at the following link:
                <a href="https://drive.google.com/drive/u/4/folders/11-dnlbMjm8O_ynq1REuDYKOmHLqEhGYP">full BRAVO
                    Dataset download link</a>.
            </p>
            <p>The dataset includes the following splits (with individual download links):</p>
            <p><b>bravo-synobjs:</b> augmented scenes with inpainted synthetic OOD objects. We augmented the
                validation images of Cityscapes and generated 656 images with 26 OOD objects. (<a
                    href="https://drive.google.com/drive/u/4/folders/1KKt_25S69DBf8ZTxhOhELpLgS2gyyGnf">download
                    link</a>)</p>
            </p>
            <p class="image-container">
                <img src="images/bravobenchmark/synobjs/cheetah.png" alt="Image 1">
                <img src="images/bravobenchmark/synobjs/chimpanzee.png" alt="Image 2">
                <img src="images/bravobenchmark/synobjs/lion.png" alt="Image 3">
                <img src="images/bravobenchmark/synobjs/panda.png" alt="Image 4">
                <img src="images/bravobenchmark/synobjs/penguine.png" alt="Image 5">
            </p>
            <p><b>bravo-synrain:</b> augmented scenes with synthesized raindrops on the camera lens. We
                augmented the validation images of Cityscapes and generated 500 images with raindrops. (<a
                    href="https://drive.google.com/drive/u/4/folders/1onP6tUVSjV-qKWWLm6wiOZCB9U14_gQ6">download
                    link</a>)</p>
            <p class="image-container">
                <img src="images/bravobenchmark/synrain/rain1.png" alt="Image 1">
                <img src="images/bravobenchmark/synrain/rain2.png" alt="Image 2">
                <img src="images/bravobenchmark/synrain/rain3.png" alt="Image 3">
                <img src="images/bravobenchmark/synrain/rain4.png" alt="Image 4">
                <img src="images/bravobenchmark/synrain/rain5.png" alt="Image 5">
            </p>
            <p><b>bravo-synflare:</b> augmented scenes with synthesized light flares. We augmented the
                validation images of Cityscapes and generated 308 images with random light flares. (<a
                    href="https://drive.google.com/drive/u/4/folders/13EpBXUY8BChoqfMxR5JhiyhqrzqLAO2y">download
                    link</a>)</p>
            <p class="image-container">
                <img src="images/bravobenchmark/synflare/flare1.png" alt="Image 1">
                <img src="images/bravobenchmark/synflare/flare2.png" alt="Image 2">
                <img src="images/bravobenchmark/synflare/flare3.png" alt="Image 3">
                <img src="images/bravobenchmark/synflare/flare4.png" alt="Image 4">
                <img src="images/bravobenchmark/synflare/flare5.png" alt="Image 5">
            </p>
            <p><b>bravo-outofcontext:</b> augmented scenes with random backgrounds. We augmented the
                validation images of Cityscapes and generated 329 images with random random backgrounds. (<a
                    href="https://drive.google.com/drive/u/4/folders/1NoXqTQWxrj_yKMNRKLOd1rnn2TjqIaU5">download
                    link</a>)</p>
            <p class="image-container">
                <img src="images/bravobenchmark/synooc/ooc1.png" alt="Image 1">
                <img src="images/bravobenchmark/synooc/ooc2.png" alt="Image 2">
                <img src="images/bravobenchmark/synooc/ooc3.png" alt="Image 3">
                <img src="images/bravobenchmark/synooc/ooc4.png" alt="Image 4">
                <img src="images/bravobenchmark/synooc/ooc5.png" alt="Image 5">
            </p>
            <p><b>bravo-ACDC:</b> real scenes captured in adverse weather conditions, i.e. fog, night, rain
                and snow. (<a
                    href="https://drive.google.com/drive/u/4/folders/1IW6-Tdfk2At6CrIIrA-QJF6CEcHgqqha">download
                    link</a> or directly from <a href="https://acdc.vision.ee.ethz.ch/download">ACDC
                    website</a>)</p>
            <p class="image-container">
                <img src="images/bravobenchmark/acdc/acdc1.png" alt="Image 1">
                <img src="images/bravobenchmark/acdc/acdc2.png" alt="Image 2">
                <img src="images/bravobenchmark/acdc/acdc3.png" alt="Image 3">
                <img src="images/bravobenchmark/acdc/acdc4.png" alt="Image 4">
                <img src="images/bravobenchmark/acdc/acdc5.png" alt="Image 5">
            </p>
            <p><b>bravo-SMIYC:</b> real scenes featuring out-of-distribution (OOD) objects rarely encountered
                on the road. (<a
                    href="https://drive.google.com/drive/u/4/folders/1XnC9_7RzwZCWaDpP3iETbGt7Yvmg0MOg">download
                    link</a> or directly from <a href="https://segmentmeifyoucan.com/">SMIYC
                    website</a>)</p>
            <p class="image-container">
                <img src="images/bravobenchmark/smiyc/smiyc1.jpg" alt="Image 1">
                <img src="images/bravobenchmark/smiyc/smiyc2.jpg" alt="Image 2">
                <img src="images/bravobenchmark/smiyc/smiyc3.jpg" alt="Image 3">
                <img src="images/bravobenchmark/smiyc/smiyc4.jpg" alt="Image 4">
                <img src="images/bravobenchmark/smiyc/smiyc5.jpg" alt="Image 5">
            </p>

            <h3>Metrics</h3>
            <p>For a comprehensive assessment of the robustness of various semantic segmentation models, we adopt the
                following metrics:</p>
            <ul>
                <li><em>mIoU</em>: mean Intersection Over Union, quantifying the degree of overlap between correct
                    predictions and actual labels against the total number of true positives, false positives, and false
                    negatives.</li>
                <li><em>AUROC</em>: Area Under the ROC Curve, a threshold-free metric quantifying the probability
                    that a randomly chosen certain example will be ranked higher than a randomly chosen uncertain one.
                </li>
                <li><em>AUPR-Success</em>: Area Under the Precision-Recall curve.</li>
                <li><em>AUPR-Error</em>: Area Under the Precision-Recall curve Error, computing the area under the
                    Precision-Recall curve using errors as the positive class.</li>
                <li><em>FPR@95TPR</em>: measuring the False Positive Rate when setting the True Positive Rate to 95%.
                </li>
                <li><em>ECE</em>: Expected Calibration Error, measuring the expected difference between accuracy and
                    predicted uncertainty.</li>
            </ul>

            <h3>Benchmark server, Submission details, Schedule</h3>
            <p>We are excited to unveil the BRAVO Challenge as an initiative within
                <a href="https://www.elsa-ai.eu/">ELSA ‚Äî European Lighthouse on Secure and Safe AI</a>,
                a network of excellence funded by the European Union. The BRAVO Challenge is officially featured on the
                <a href="https://benchmarks.elsa-ai.eu/">ELSA Benchmarks website</a> as
                the Autonomous Driving/Robust Perception task.
            </p>
            <p>Please refer to the <a href="https://benchmarks.elsa-ai.eu/?ch=1&com=introduction">task website</a>
                for detailed submission information on the submission format and schedule.</p>

            <h3>Acknowledgements</h3>
            <p>We extend our heartfelt gratitude to the authors of
                <a href="https://acdc.vision.ee.ethz.ch/contact/">ACDC</a>,
                <a href="https://segmentmeifyoucan.com/">SegmentMeIfYouCan</a> and
                <a href="https://arxiv.org/abs/2108.00968">Out-of-context Cityscapes</a> for generously granting us
                permission to repurpose their benchmarking data. We are also thankful to the authors of
                <a href="https://github.com/astra-vision/GuidedDisent">GuidedDisent</a> and
                <a href="https://github.com/google-research/google-research/tree/master/flare_removal">Flare Removal</a>
                for providing the amazing toolboxes that helped synthesize realistic-looking raindrops and light
                flares. All those people collectively contributed to creating BRAVO, a unified benchmark
                for robustness in autonomous driving.
            </p>
        </div>

        <div id="organizers" class="container-md section-container">
            <h2>Organizers</h2>
            <div id="organizers_container"
                class="d-flex flex-wrap justify-content-around align-items-center person-container">
                <div class="card person-card">
                    <a class="person-link" href="https://tuanhungvu.github.io/">
                        <div class="card-img-top framed-photo"><img src="images/people/tuanhung.jpg" alt=""></div>
                        <div class="card-title person-name" class=>Tuan-Hung Vu</div>
                        <div class="card-text person-affiliation">Valeo.AI</div>
                    </a>
                </div>
                <div class="card person-card">
                    <a class="person-link" href="https://abursuc.github.io/">
                        <div class="card-img-top framed-photo"><img src="images/people/andrei.jpg" alt=""></div>
                        <div class="card-title person-name">Andrei Bursuc</div>
                        <div class="card-text person-affiliation">Valeo.AI</div>
                    </a>
                </div>
                <div class="card person-card">
                    <a class="person-link" href="https://ptrckprz.github.io/">
                        <div class="card-img-top framed-photo"><img src="images/people/patrick.jpg" alt=""></div>
                        <div class="card-title person-name">Patrick Perez</div>
                        <div class="card-text person-affiliation">Valeo.AI</div>
                    </a>
                </div>
                <div class="card person-card">
                    <a class="person-link" href="https://eduardovalle.com/">
                        <div class="card-img-top framed-photo"><img src="images/people/eduardo.jpg" alt=""></div>
                        <div class="card-title person-name">Eduardo Valle</div>
                        <div class="card-text person-affiliation">Valeo.AI</div>
                    </a>
                </div>
                <div class="card person-card">
                    <a class="person-link" href="https://vas.mpi-inf.mpg.de/dengxin/">
                        <div class="card-img-top framed-photo"><img src="images/people/dengxin.jpg" alt=""></div>
                        <div class="card-title person-name">Dengxin Dai</div>
                        <div class="card-text person-affiliation">MPI</div>
                    </a>
                </div>
                <div class="card person-card">
                    <a class="person-link" href="https://mpi-inf.mpg.de/~schiele">
                        <div class="card-img-top framed-photo"><img src="images/people/bernt.jpg" alt=""></div>
                        <div class="card-title person-name">Bernt Schiele</div>
                        <div class="card-text person-affiliation">MPI</div>
                    </a>
                </div>
                <div class="card person-card">
                    <a class="person-link" href="https://fr.linkedin.com/in/emilie-wirbel-3ba43233">
                        <div class="card-img-top framed-photo"><img src="images/people/emilie.jpg" alt=""></div>
                        <div class="card-title person-name">Emilie Wirbel</div>
                        <div class="card-text person-affiliation">NVIDIA</div>
                    </a>
                </div>
            </div>
        </div>

        <div id="supporters" class="container-md section-container">
            <p>Supported by</p>
            <div id="supporters_container"
                class="d-flex flex-wrap justify-content-left align-items-top person-container">
                <div class="card supporter-card">
                    <a class="supporter-link" href="https://elsa-ai.eu/">
                        <div class="card-body"><img class="supporter-image" src="images/logos/elsa.png" alt=""></div>
                    </a>
                </div>
            </div>
        </div>

        <div id="supporters" class="container-md section-colophon">
            <p>Original photo by Kai Gradert on Unsplash, modified to illustrate stable diffusion augmentations.</p>
        </div>

    </div>
</body>

</html>